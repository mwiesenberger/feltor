<!-- HTML header for doxygen 1.9.3-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.12.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Discontinuous Galerkin Library: Distributed Matrix and Vector</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
    <!-- ... doxygen-awesome Fragment copy ... -->
    <script type="text/javascript" src="doxygen-awesome-fragment-copy-button.js"></script>
    <script type="text/javascript">
        DoxygenAwesomeFragmentCopyButton.init()
    </script>
    <!-- ... End doxygen-awesome Fragment copy ... -->
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
window.MathJax = {
  options: {
    ignoreHtmlClass: 'tex2jax_ignore',
    processHtmlClass: 'tex2jax_process'
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-fragment-copy-button.js" rel="stylesheet" type="text/css"/>
<link href="menubar.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<!-- Make a nice link to github (copied from doxygen-awesome/doxygen-custom/header.html-->
<!-- https://tholman.com/github-corners/ -->
<a href="https://github.com/feltor-dev/feltor" class="github-corner" title="View source on GitHub" target="_blank">
    <svg viewBox="0 0 250 250" width="80" height="80" style="position: absolute; top: 0; border: 0; right: 0; z-index: 99;" aria-hidden="true">
    <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
<!-- End Make a nice link to github -->
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
    <!--My own title area-->
  <ul class="menubar">
      <!-- <li><a href="../../../index.html">/</a><li>-->
    <li><a href="../../dg/html/topics.html">dg</a></li>
    <li><a href="../../geometries/html/topics.html">dg::geo</a></li>
    <li><a href="../../file/html/topics.html">dg::file</a></li>
    <li><a href="../../exblas/html/namespacedg_1_1exblas.html">dg::exblas</a></li>
    <li><a href="../../matrix/html/topics.html">dg::mat</a></li>
  </ul>
  <!--End My own title area-->
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Discontinuous Galerkin Library
   </div>
   <div id="projectbrief">#include &quot;dg/algorithm.h&quot;</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.12.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search',true);
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('group__mpi__matvec.html',''); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">Distributed Matrix and Vector<div class="ingroups"><a class="el" href="group__level1.html">Level 1: Vectors, Matrices and basic operations</a> &raquo; <a class="el" href="group__mpi__structures.html">MPI backend</a></div></div></div>
</div><!--header-->
<div class="contents">
<div class="dynheader">
Collaboration diagram for Distributed Matrix and Vector:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="group__mpi__matvec.svg" width="303" height="51"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1_m_p_i_sparse_block_mat.html">dg::MPISparseBlockMat&lt; Vector, LocalMatrixInner, LocalMatrixOuter &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Distributed memory Sparse block matrix class, asynchronous communication.  <a href="structdg_1_1_m_p_i_sparse_block_mat.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1_m_p_i_dist_mat.html">dg::MPIDistMat&lt; Vector, LocalMatrixInner, LocalMatrixOuter &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Distributed memory matrix class, asynchronous communication.  <a href="structdg_1_1_m_p_i_dist_mat.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1_m_p_i___vector.html">dg::MPI_Vector&lt; container &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A simple wrapper around a container object and an MPI_Comm.  <a href="structdg_1_1_m_p_i___vector.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:ga5bae2f49640615237a95bc7e50d42231" id="r_ga5bae2f49640615237a95bc7e50d42231"><td class="memTemplParams" colspan="2">template&lt;class real_type , class ConversionPolicyRows , class ConversionPolicyCols &gt; </td></tr>
<tr class="memitem:ga5bae2f49640615237a95bc7e50d42231"><td class="memTemplItemLeft" align="right" valign="top">auto&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="#ga5bae2f49640615237a95bc7e50d42231">dg::make_mpi_sparseblockmat</a> (const <a class="el" href="structdg_1_1_ell_sparse_block_mat.html">EllSparseBlockMat</a>&lt; real_type, thrust::host_vector &gt; &amp;src, const ConversionPolicyRows &amp;g_rows, const ConversionPolicyCols &amp;g_cols)</td></tr>
<tr class="memdesc:ga5bae2f49640615237a95bc7e50d42231"><td class="mdescLeft">&#160;</td><td class="mdescRight">Split given <code><a class="el" href="structdg_1_1_ell_sparse_block_mat.html" title="Ell Sparse Block Matrix format.">EllSparseBlockMat</a></code> into computation and communication part.  <br /></td></tr>
<tr class="separator:ga5bae2f49640615237a95bc7e50d42231"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga9057cdd9d1b8f55ed00a91eca7781cb2" id="r_ga9057cdd9d1b8f55ed00a91eca7781cb2"><td class="memTemplParams" colspan="2">template&lt;class ConversionPolicy , class real_type &gt; </td></tr>
<tr class="memitem:ga9057cdd9d1b8f55ed00a91eca7781cb2"><td class="memTemplItemLeft" align="right" valign="top"><a class="el" href="group__typedefs.html#ga52339ea3efd3f2a55110c7bd63b17c9d">dg::MIHMatrix_t</a>&lt; real_type &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="#ga9057cdd9d1b8f55ed00a91eca7781cb2">dg::make_mpi_matrix</a> (const <a class="el" href="group__typedefs.html#gad1abe3c0871fb2df8a63e9d7904fe2b5">dg::IHMatrix_t</a>&lt; real_type &gt; &amp;global_cols, const ConversionPolicy &amp;col_policy)</td></tr>
<tr class="memdesc:ga9057cdd9d1b8f55ed00a91eca7781cb2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Convert a (row-distributed) matrix with local row and global column indices to a row distributed MPI matrix.  <br /></td></tr>
<tr class="separator:ga9057cdd9d1b8f55ed00a91eca7781cb2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga7f829bc67892929b7a9727e5d8202c1d" id="r_ga7f829bc67892929b7a9727e5d8202c1d"><td class="memTemplParams" colspan="2">template&lt;class ConversionPolicy , class real_type &gt; </td></tr>
<tr class="memitem:ga7f829bc67892929b7a9727e5d8202c1d"><td class="memTemplItemLeft" align="right" valign="top"><a class="el" href="group__typedefs.html#gad1abe3c0871fb2df8a63e9d7904fe2b5">dg::IHMatrix_t</a>&lt; real_type &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="#ga7f829bc67892929b7a9727e5d8202c1d">dg::convertGlobal2LocalRows</a> (const <a class="el" href="group__typedefs.html#gad1abe3c0871fb2df8a63e9d7904fe2b5">dg::IHMatrix_t</a>&lt; real_type &gt; &amp;global, const ConversionPolicy &amp;row_policy)</td></tr>
<tr class="memdesc:ga7f829bc67892929b7a9727e5d8202c1d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Convert a (column-distributed) matrix with global row and column indices to a row distributed matrix.  <br /></td></tr>
<tr class="separator:ga7f829bc67892929b7a9727e5d8202c1d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga0507e33485abf1637aaf947dc2876bfe" id="r_ga0507e33485abf1637aaf947dc2876bfe"><td class="memTemplParams" colspan="2">template&lt;class ConversionPolicy , class real_type &gt; </td></tr>
<tr class="memitem:ga0507e33485abf1637aaf947dc2876bfe"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="#ga0507e33485abf1637aaf947dc2876bfe">dg::convertLocal2GlobalCols</a> (<a class="el" href="group__typedefs.html#gad1abe3c0871fb2df8a63e9d7904fe2b5">dg::IHMatrix_t</a>&lt; real_type &gt; &amp;local, const ConversionPolicy &amp;policy)</td></tr>
<tr class="memdesc:ga0507e33485abf1637aaf947dc2876bfe"><td class="mdescLeft">&#160;</td><td class="mdescRight">Convert a matrix with local column indices to a matrix with global column indices.  <br /></td></tr>
<tr class="separator:ga0507e33485abf1637aaf947dc2876bfe"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<h1><a class="anchor" id="mpi_matrix"></a>
MPI Matrices and the symv function</h1>
<p>Contrary to a vector a matrix can be distributed among processes in two ways: <em>row-wise</em> and <em>column-wise</em>. When we implement a matrix-vector multiplication the order of communication and computation depends on the distribution of the matrix. </p><dl class="section note"><dt>Note</dt><dd>The dg library only implements row distributed matrices. The reason is that the associated matrix-vector product can be made binary reproducible. Support for column distributed matrices was dropped.</dd></dl>
<h2><a class="anchor" id="mpi_row"></a>
Row distributed matrices</h2>
<p>In a row-distributed matrix each process holds the rows of the matrix that correspond to the portion of the <code><a class="el" href="structdg_1_1_m_p_i___vector.html" title="A simple wrapper around a container object and an MPI_Comm.">MPI_Vector</a></code> it holds. When we implement a matrix-vector multiplication each process first has to gather all the elements of the input vector it needs to be able to compute the elements of its output. In general this requires MPI communication. (s.a. <a class="el" href="group__mpi__comm.html#mpigather">MPI distributed gather and scatter operations</a> for more info of how global scatter/gather operations work). After the elements have been gathered into a buffer the local matrix-vector multiplications can be executed. Formally, the gather operation can be written as a matrix \(G\) of \(1&#39;\)s and \(0&#39;\)s and we write.   </p><p class="formulaDsp">
\[
M v = R\cdot G v
\]
</p>
<p> where \(R\) is the row-distributed matrix with modified indices into a buffer vector and \(G\) is the gather matrix, in which the MPI-communication takes place. In this way we achieve a simple split between communication \( w=Gv\) and computation \( Rw\). Since the computation of \( R w\) is entirely local we can reuse the existing implementation for shared memory systems.</p>
<h2><a class="anchor" id="mpi_row_col"></a>
Separation of communication and computation</h2>
<p>We can go one step further on a row distributed matrix and separate the matrix \( M \) into   </p><p class="formulaDsp">
\[
M v = (M_i + M_o) v = (M_{i} + R_o\cdot G_o) v
\]
</p>
<p> where \( M_i\) is the inner matrix which requires no communication, while \( M_o\) is the outer matrix containing all elements which require MPI communication. This enables the implementation of overlapping communication and computation which is done in the <code><a class="el" href="structdg_1_1_m_p_i_dist_mat.html" title="Distributed memory matrix class, asynchronous communication.">dg::MPIDistMat</a></code> and <code><a class="el" href="structdg_1_1_m_p_i_sparse_block_mat.html" title="Distributed memory Sparse block matrix class, asynchronous communication.">dg::MPISparseBlockMat</a></code> classes. </p>
<h2><a class="anchor" id="mpi_create"></a>
Creation</h2>
<p>You can create a row-distributed MPI matrix given its local parts on each process with local row and global column indices by our <code><a class="el" href="#ga9057cdd9d1b8f55ed00a91eca7781cb2" title="Convert a (row-distributed) matrix with local row and global column indices to a row distributed MPI ...">dg::make_mpi_matrix</a></code> function. If you have a column distributed matrix with its local parts on each process with global row and local columns indices, you can use a combination of <code><a class="el" href="#ga0507e33485abf1637aaf947dc2876bfe" title="Convert a matrix with local column indices to a matrix with global column indices.">dg::convertLocal2GlobalCols</a></code> and <code><a class="el" href="#ga7f829bc67892929b7a9727e5d8202c1d" title="Convert a (column-distributed) matrix with global row and column indices to a row distributed matrix.">dg::convertGlobal2LocalRows</a></code> to bring it to a row-distributed form. The latter can then be used in <code><a class="el" href="#ga9057cdd9d1b8f55ed00a91eca7781cb2" title="Convert a (row-distributed) matrix with local row and global column indices to a row distributed MPI ...">dg::make_mpi_matrix</a></code> again.</p>
<h2><a class="anchor" id="mpi_column"></a>
Column distributed matrices</h2>
<p>In a column-distributed matrix each process holds the columns of the matrix that correspond to the portion of the <code><a class="el" href="structdg_1_1_m_p_i___vector.html" title="A simple wrapper around a container object and an MPI_Comm.">MPI_Vector</a></code> it holds. In a column distributed matrix the local matrix-vector multiplication can be executed first because each processor already has all vector elements it needs. However the resulting elements have to be communicated back to the process they belong to. Furthermore, a process has to sum all elements it receives from other processes on the same index. This is a scatter and reduce operation and it can be written as a scatter matrix \(S\)   </p><p class="formulaDsp">
\[
M v= S\cdot C v
\]
</p>
<p> where \(S\) is the scatter matrix and \(C\) is the column distributed matrix with modified indices. Again, we can reuse our shared memory algorithms to implement the local matrix-vector operation \( w=Cv\) before the communication step \( S w\). </p>
<h2><a class="anchor" id="mpi_transpose"></a>
Transposition</h2>
<p>It turns out that a row-distributed matrix can be transposed by transposition of both the local matrix and the gather matrix:  </p><p class="formulaDsp">
\[
M^\mathrm{T} = G^\mathrm{T} R^\mathrm{T} = S C\]
</p>
<p> The result is then a column distributed matrix. Analogously, the transpose of a column distributed matrix is a row-distributed matrix. It is also possible to convert a column distributed mpi matrix to a row distributed mpi matrix. In code </p><div class="fragment"><div class="line"><span class="comment">// Tranpose a row distributed matrix to another row distributed matrix</span></div>
<div class="line"><a class="code hl_struct" href="structdg_1_1_sparse_matrix.html">dg::IHMatrix</a> <a class="code hl_variableRef" href="../../matrix/html/exp__runge__kutta__t_8cpp.html#a1fdef7b422c652726a6b5d351d8d1a4d">matrix</a>;</div>
<div class="line"><span class="comment">//...</span></div>
<div class="line"><span class="comment">// Suppose we have row distributed matrix with local rows and global cols</span></div>
<div class="line"><a class="code hl_struct" href="structdg_1_1_sparse_matrix.html">dg::IHMatrix</a> matrixT= <a class="code hl_variableRef" href="../../matrix/html/exp__runge__kutta__t_8cpp.html#a1fdef7b422c652726a6b5d351d8d1a4d">matrix</a>.transpose();</div>
<div class="line"><span class="comment">// matrixT is column distributed</span></div>
<div class="line"><span class="comment">// matrixT has global rows and local column indices</span></div>
<div class="line"><a class="code hl_function" href="#ga0507e33485abf1637aaf947dc2876bfe">dg::convertLocal2GlobalCols</a>( matrixT, grid);</div>
<div class="line"><span class="comment">// now matrixT has global rows and global column indices</span></div>
<div class="line"><span class="keyword">auto</span> mat = <a class="code hl_function" href="#ga7f829bc67892929b7a9727e5d8202c1d">dg::convertGlobal2LocalRows</a>( matrixT, grid);</div>
<div class="line"><span class="comment">// now mat is row distributed with global column indices</span></div>
<div class="line"><span class="keyword">auto</span> mpi_mat = <a class="code hl_function" href="#ga9057cdd9d1b8f55ed00a91eca7781cb2">dg::make_mpi_matrix</a>(  mat, grid);</div>
<div class="ttc" id="aexp__runge__kutta__t_8cpp_html_a1fdef7b422c652726a6b5d351d8d1a4d"><div class="ttname"><a href="../../matrix/html/exp__runge__kutta__t_8cpp.html#a1fdef7b422c652726a6b5d351d8d1a4d">matrix</a></div><div class="ttdeci">const double matrix</div></div>
<div class="ttc" id="agroup__mpi__matvec_html_ga0507e33485abf1637aaf947dc2876bfe"><div class="ttname"><a href="#ga0507e33485abf1637aaf947dc2876bfe">dg::convertLocal2GlobalCols</a></div><div class="ttdeci">void convertLocal2GlobalCols(dg::IHMatrix_t&lt; real_type &gt; &amp;local, const ConversionPolicy &amp;policy)</div><div class="ttdoc">Convert a matrix with local column indices to a matrix with global column indices.</div><div class="ttdef"><b>Definition</b> mpi_projection.h:230</div></div>
<div class="ttc" id="agroup__mpi__matvec_html_ga7f829bc67892929b7a9727e5d8202c1d"><div class="ttname"><a href="#ga7f829bc67892929b7a9727e5d8202c1d">dg::convertGlobal2LocalRows</a></div><div class="ttdeci">dg::IHMatrix_t&lt; real_type &gt; convertGlobal2LocalRows(const dg::IHMatrix_t&lt; real_type &gt; &amp;global, const ConversionPolicy &amp;row_policy)</div><div class="ttdoc">Convert a (column-distributed) matrix with global row and column indices to a row distributed matrix.</div><div class="ttdef"><b>Definition</b> mpi_projection.h:165</div></div>
<div class="ttc" id="agroup__mpi__matvec_html_ga9057cdd9d1b8f55ed00a91eca7781cb2"><div class="ttname"><a href="#ga9057cdd9d1b8f55ed00a91eca7781cb2">dg::make_mpi_matrix</a></div><div class="ttdeci">dg::MIHMatrix_t&lt; real_type &gt; make_mpi_matrix(const dg::IHMatrix_t&lt; real_type &gt; &amp;global_cols, const ConversionPolicy &amp;col_policy)</div><div class="ttdoc">Convert a (row-distributed) matrix with local row and global column indices to a row distributed MPI ...</div><div class="ttdef"><b>Definition</b> mpi_projection.h:50</div></div>
<div class="ttc" id="astructdg_1_1_sparse_matrix_html"><div class="ttname"><a href="structdg_1_1_sparse_matrix.html">dg::SparseMatrix</a></div><div class="ttdoc">A CSR formatted sparse matrix.</div><div class="ttdef"><b>Definition</b> sparsematrix.h:96</div></div>
</div><!-- fragment --><h2 class="groupheader">Function Documentation</h2>
<a id="ga7f829bc67892929b7a9727e5d8202c1d" name="ga7f829bc67892929b7a9727e5d8202c1d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga7f829bc67892929b7a9727e5d8202c1d">&#9670;&#160;</a></span>convertGlobal2LocalRows()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class ConversionPolicy , class real_type &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group__typedefs.html#gad1abe3c0871fb2df8a63e9d7904fe2b5">dg::IHMatrix_t</a>&lt; real_type &gt; dg::convertGlobal2LocalRows </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group__typedefs.html#gad1abe3c0871fb2df8a63e9d7904fe2b5">dg::IHMatrix_t</a>&lt; real_type &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>global</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const ConversionPolicy &amp;</td>          <td class="paramname"><span class="paramname"><em>row_policy</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Convert a (column-distributed) matrix with global row and column indices to a row distributed matrix. </p>
<p>Send all elements with a global row-index that does not belong to the calling process to the process where it belongs to. This can be used to convert a column distributed matrix to a row-distributed matrix as in </p><div class="fragment"><div class="line"><a class="code hl_struct" href="structdg_1_1_sparse_matrix.html">dg::IHMatrix_t&lt;real_type&gt;</a> mat = <a class="code hl_function" href="group__interpolation.html#ga9383cf8ef0d0cc6af8c1250ecb31dc58">dg::create::projection</a>(</div>
<div class="line">    g_new.global(), g_old.local(), method);</div>
<div class="line"><span class="comment">// mat is column distributed</span></div>
<div class="line"><span class="comment">// mat has global rows and local column indices</span></div>
<div class="line"><a class="code hl_function" href="#ga0507e33485abf1637aaf947dc2876bfe">dg::convertLocal2GlobalCols</a>( mat, g_old);</div>
<div class="line"><span class="comment">// now mat has global rows and global column indices</span></div>
<div class="line"><span class="keyword">auto</span> mat_loc = <a class="code hl_function" href="#ga7f829bc67892929b7a9727e5d8202c1d">dg::convertGlobal2LocalRows</a>( mat, g_new);</div>
<div class="line"><span class="comment">// now mat is row distributed with global column indices</span></div>
<div class="line"><span class="keyword">auto</span> mpi_mat = <a class="code hl_function" href="#ga9057cdd9d1b8f55ed00a91eca7781cb2">dg::make_mpi_matrix</a>(  mat_loc, g_old);</div>
<div class="ttc" id="agroup__interpolation_html_ga9383cf8ef0d0cc6af8c1250ecb31dc58"><div class="ttname"><a href="group__interpolation.html#ga9383cf8ef0d0cc6af8c1250ecb31dc58">dg::create::projection</a></div><div class="ttdeci">dg::MIHMatrix_t&lt; typename MPITopology::value_type &gt; projection(const MPITopology &amp;g_new, const MPITopology &amp;g_old, std::string method=&quot;dg&quot;)</div><div class="ttdoc">Create a projection between two grids.</div><div class="ttdef"><b>Definition</b> mpi_projection.h:272</div></div>
</div><!-- fragment --> <dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">ConversionPolicy</td><td>(can be one of the MPI grids ) has to have the members:<ul>
<li><code> bool global2localIdx(unsigned,unsigned&amp;,unsigned&amp;) const; </code> where the first parameter is the global index and the other two are the output pair (localIdx, rank). return true if successful, false if global index is not part of the grid</li>
<li><code> MPI_Comm communicator() const; </code> returns the communicator to use in the gather/scatter</li>
<li><code> local_size(); </code> return the local vector size </li>
</ul>
</td></tr>
  </table>
  </dd>
</dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">global</td><td>the row indices and num_rows need to be global </td></tr>
    <tr><td class="paramname">row_policy</td><td>the conversion object</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>a row distributed MPI matrix. If no MPI communication is needed it simply has row-indices converted from global to local indices. <code>num_cols</code> is the one from <code>global</code> </dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="group__basictopology.html">Discontinuous Galerkin Nd grid</a> the MPI grids defined in Level 3 can all be used as a ConversionPolicy; </dd></dl>

</div>
</div>
<a id="ga0507e33485abf1637aaf947dc2876bfe" name="ga0507e33485abf1637aaf947dc2876bfe"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga0507e33485abf1637aaf947dc2876bfe">&#9670;&#160;</a></span>convertLocal2GlobalCols()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class ConversionPolicy , class real_type &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">void dg::convertLocal2GlobalCols </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group__typedefs.html#gad1abe3c0871fb2df8a63e9d7904fe2b5">dg::IHMatrix_t</a>&lt; real_type &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>local</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const ConversionPolicy &amp;</td>          <td class="paramname"><span class="paramname"><em>policy</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Convert a matrix with local column indices to a matrix with global column indices. </p>
<p>Simply call policy.local2globalIdx for every column index </p><div class="fragment"><div class="line"><a class="code hl_struct" href="structdg_1_1_sparse_matrix.html">dg::IHMatrix_t&lt;real_type&gt;</a> mat = <a class="code hl_function" href="group__interpolation.html#ga9383cf8ef0d0cc6af8c1250ecb31dc58">dg::create::projection</a>(</div>
<div class="line">    g_new.global(), g_old.local(), method);</div>
<div class="line"><span class="comment">// mat is column distributed</span></div>
<div class="line"><span class="comment">// mat has global rows and local column indices</span></div>
<div class="line"><a class="code hl_function" href="#ga0507e33485abf1637aaf947dc2876bfe">dg::convertLocal2GlobalCols</a>( mat, g_old);</div>
<div class="line"><span class="comment">// now mat has global rows and global column indices</span></div>
<div class="line"><span class="keyword">auto</span> mat_loc = <a class="code hl_function" href="#ga7f829bc67892929b7a9727e5d8202c1d">dg::convertGlobal2LocalRows</a>( mat, g_new);</div>
<div class="line"><span class="comment">// now mat is row distributed with global column indices</span></div>
<div class="line"><span class="keyword">auto</span> mpi_mat = <a class="code hl_function" href="#ga9057cdd9d1b8f55ed00a91eca7781cb2">dg::make_mpi_matrix</a>(  mat_loc, g_old);</div>
</div><!-- fragment --> <dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">ConversionPolicy</td><td>(can be one of the MPI grids ) has to have the members:<ul>
<li><code>bool local2globalIdx(unsigned,unsigned&amp;,unsigned&amp;) const;</code> where the first two parameters are the pair (localIdx, rank). and the last one is the global index and the return true if successful, false if index is not part of the grid</li>
<li><code>unsigned size() const;</code> returns what will become the new <code>num_cols</code> </li>
</ul>
</td></tr>
  </table>
  </dd>
</dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">local</td><td>the column indices and num_cols need to be local, will be global on output </td></tr>
    <tr><td class="paramname">policy</td><td>the conversion object</td></tr>
  </table>
  </dd>
</dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="group__basictopology.html">Discontinuous Galerkin Nd grid</a> the MPI grids defined in Level 3 can all be used as a ConversionPolicy </dd></dl>

</div>
</div>
<a id="ga9057cdd9d1b8f55ed00a91eca7781cb2" name="ga9057cdd9d1b8f55ed00a91eca7781cb2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga9057cdd9d1b8f55ed00a91eca7781cb2">&#9670;&#160;</a></span>make_mpi_matrix()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class ConversionPolicy , class real_type &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group__typedefs.html#ga52339ea3efd3f2a55110c7bd63b17c9d">dg::MIHMatrix_t</a>&lt; real_type &gt; dg::make_mpi_matrix </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group__typedefs.html#gad1abe3c0871fb2df8a63e9d7904fe2b5">dg::IHMatrix_t</a>&lt; real_type &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>global_cols</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const ConversionPolicy &amp;</td>          <td class="paramname"><span class="paramname"><em>col_policy</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Convert a (row-distributed) matrix with local row and global column indices to a row distributed MPI matrix. </p>
<div class="fragment"><div class="line"><a class="code hl_struct" href="structdg_1_1_sparse_matrix.html">dg::IHMatrix_t&lt;real_type&gt;</a> mat = <a class="code hl_function" href="group__interpolation.html#gaebbccbc4e9b740740cd2e9db394d580f">dg::create::interpolation</a>(</div>
<div class="line">    g_new.local(), g_old.global(), method);</div>
<div class="line"><span class="comment">// mat is row distributed</span></div>
<div class="line"><span class="comment">// mat has local row and global column indices</span></div>
<div class="line"><span class="keyword">auto</span> mpi_mat = <a class="code hl_function" href="#ga9057cdd9d1b8f55ed00a91eca7781cb2">dg::make_mpi_matrix</a>(  mat, g_old);</div>
<div class="ttc" id="agroup__interpolation_html_gaebbccbc4e9b740740cd2e9db394d580f"><div class="ttname"><a href="group__interpolation.html#gaebbccbc4e9b740740cd2e9db394d580f">dg::create::interpolation</a></div><div class="ttdeci">dg::SparseMatrix&lt; int, real_type, thrust::host_vector &gt; interpolation(const RecursiveHostVector &amp;x, const aRealTopology&lt; real_type, Nd &gt; &amp;g, std::array&lt; dg::bc, Nd &gt; bcx, std::string method=&quot;dg&quot;)</div><div class="ttdoc">Create interpolation matrix of a list of points in given grid.</div><div class="ttdef"><b>Definition</b> interpolation.h:433</div></div>
</div><!-- fragment --> <dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">ConversionPolicy</td><td>(can be one of the MPI grids ) has to have the members:<ul>
<li><code>bool global2localIdx(unsigned,unsigned&amp;,unsigned&amp;) const;</code> where the first parameter is the global index and the other two are the output pair (localIdx, rank). return true if successful, false if global index is not part of the grid</li>
<li><code>MPI_Comm communicator() const;</code> returns the communicator to use in the gather/scatter</li>
<li><code>unsigned local_size() const;</code> return the local vector size</li>
</ul>
</td></tr>
  </table>
  </dd>
</dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">global_cols</td><td>the local part of the matrix (different on each process) with <b>global column indices</b> and num_cols but <b>local row indices</b> and num_rows </td></tr>
    <tr><td class="paramname">col_policy</td><td>the conversion object</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>a row distributed MPI matrix. If no MPI communication is needed the collective communicator will have zero size. </dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="group__basictopology.html">Discontinuous Galerkin Nd grid</a> the MPI grids defined in Level 3 can all be used as a ConversionPolicy </dd></dl>

</div>
</div>
<a id="ga5bae2f49640615237a95bc7e50d42231" name="ga5bae2f49640615237a95bc7e50d42231"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga5bae2f49640615237a95bc7e50d42231">&#9670;&#160;</a></span>make_mpi_sparseblockmat()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class real_type , class ConversionPolicyRows , class ConversionPolicyCols &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">auto dg::make_mpi_sparseblockmat </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structdg_1_1_ell_sparse_block_mat.html">EllSparseBlockMat</a>&lt; real_type, thrust::host_vector &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>src</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const ConversionPolicyRows &amp;</td>          <td class="paramname"><span class="paramname"><em>g_rows</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const ConversionPolicyCols &amp;</td>          <td class="paramname"><span class="paramname"><em>g_cols</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Split given <code><a class="el" href="structdg_1_1_ell_sparse_block_mat.html" title="Ell Sparse Block Matrix format.">EllSparseBlockMat</a></code> into computation and communication part. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">src</td><td>global rows and global cols, right_size and left_size must have correct local sizes </td></tr>
    <tr><td class="paramname">g_rows</td><td>1 dimensional grid for the rows Is needed for <code>local2globalIdx</code> and <code>global2localIdx</code> </td></tr>
    <tr><td class="paramname">g_cols</td><td>1 dimensional grid for the columns </td></tr>
  </table>
  </dd>
</dl>
<dl class="section attention"><dt>Attention</dt><dd>communicators in <code>g_rows</code> and <code>g_cols</code> need to be at least <code>MPI_CONGRUENT</code> </dd></dl>
<dl class="section return"><dt>Returns</dt><dd>MPI distributed spares block matrix of type <code><a class="el" href="group__typedefs.html#gafe5fb1d8acc7f155c39d6b2edfebf782" title="MPI Host Matrix for derivatives.">dg::MHMatrix_t&lt;real_type&gt;</a></code> </dd></dl>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.9.3-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Mon Jun 23 2025 12:36:30 for Discontinuous Galerkin Library by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.12.0 </li>
  </ul>
</div>
</body>
</html>
